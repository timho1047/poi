#!/usr/bin/env python3
"""
Calculate Unique SID from RQVAE Codebook CSV files

This script reads codebook CSV files from Hugging Face (containing Pid, Codebook, Vector)
and calculates the number of unique SIDs generated by quantization.

USAGE:
    # Batch process all models (default)
    uv run scripts/evaluate/calculate_unique_sid_from_json.py
    
    # From local CSV file
    uv run scripts/evaluate/calculate_unique_sid_from_json.py \
        --local-file path/to/codebooks-rqvae-nyc-div0.0-commit0.5-lr5e-5.csv
"""

import argparse
import ast
import csv
import pandas as pd
from pathlib import Path
from typing import Set
from huggingface_hub import hf_hub_download
from poi.settings import HF_ORG, HF_TOKEN


def codebook_to_sid(codebook_list) -> str:
    """
    Convert codebook list to SID string format.
    Uses the same format as llm_dataset_generator.py.
    
    Args:
        codebook_list: List of integers or string representation of list
                      (e.g., [4, 17, 25] or "[4, 17, 25]")
        
    Returns:
        SID string format (e.g., "<a_4><b_17><c_25>")
    """
    if isinstance(codebook_list, str):
        # If it's a string representation of list, parse it
        try:
            codebook_list = ast.literal_eval(codebook_list)
        except:
            codebook_list = []
    
    # Convert to SID format: same as llm_dataset_generator.py
    # Format: <a_20><b_21><c_19> using chr(97 + idx) for 'a', 'b', 'c', ...
    return ''.join([f"<{chr(97 + idx)}_{code}>" for idx, code in enumerate(codebook_list)])


def calculate_metrics_from_csv(df: pd.DataFrame) -> dict:
    """
    Calculate unique SID and collision metrics from CSV DataFrame.
    
    Args:
        df: DataFrame with columns ['Pid', 'Codebook', 'Vector']
            where 'Codebook' is a list of integers representing the SID
        
    Returns:
        Dictionary with metrics: 'unique_sid', 'collided_sid_buckets', 'total_pois'
    """
    df = df.copy()
    
    # Parse Codebook if it's a string
    df['Codebook'] = df['Codebook'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)
    
    # Convert Codebook list to SID string format
    df['SID'] = df['Codebook'].apply(codebook_to_sid)
    
    # Count unique SIDs
    unique_sid_count = df['SID'].nunique()
    
    # Calculate collision metrics
    # Count how many unique POIs each SID maps to
    sid_counts = df.groupby('SID')['Pid'].nunique()
    
    # Collided SID buckets: SIDs that are shared by 2+ POIs
    collided_sid_buckets = int((sid_counts >= 2).sum())
    
    return {
        'unique_sid': unique_sid_count,
        'collided_sid_buckets': collided_sid_buckets,
        'total_pois': len(df)
    }


def load_csv_from_hf(dataset_name: str, model_name: str) -> pd.DataFrame:
    """Load codebook CSV file from Hugging Face."""
    filename = f"codebooks-{model_name}.csv"
    repo_id = f"{HF_ORG}/{dataset_name.lower()}"
    hf_path = f"LLM Dataset/Intermediate Files/{filename}"
    
    print(f"   üì• Downloading: {hf_path}")
    
    local_path = hf_hub_download(
        repo_id=repo_id,
        repo_type="dataset",
        filename=hf_path,
        token=HF_TOKEN
    )
    
    # Read CSV file
    df = pd.read_csv(local_path)
    return df


def load_csv_from_local(file_path: str) -> pd.DataFrame:
    """Load CSV file from local path."""
    print(f"üìÇ Loading from local file: {file_path}")
    
    df = pd.read_csv(file_path)
    return df


def process_model(dataset_name: str, model_name: str) -> dict:
    """Process a single model and return metrics."""
    try:
        df = load_csv_from_hf(dataset_name, model_name)
        metrics = calculate_metrics_from_csv(df)
        return metrics
    except Exception as e:
        print(f"   ‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    parser = argparse.ArgumentParser(
        description="Calculate unique SID count from codebook CSV files"
    ) 
    parser.add_argument(
        '--local-file',
        type=str,
        help='Path to local CSV file (alternative to HF download)'
    )
    
    args = parser.parse_args()
    
    # If local file is provided, process it only
    if args.local_file:
        if not Path(args.local_file).exists():
            print(f"‚ùå File not found: {args.local_file}")
            return
        
        print("üöÄ Processing Local CSV File")
        print("=" * 70)
        df = load_csv_from_local(args.local_file)
        metrics = calculate_metrics_from_csv(df)
        
        print(f"\nüìà Results")
        print("=" * 70)
        print(f"Unique SID count: {metrics['unique_sid']}")
        print(f"Collided SID buckets: {metrics['collided_sid_buckets']}")
        print(f"Total POIs: {metrics['total_pois']}")
        return
    
    # Batch process all models from Hugging Face
    if not HF_TOKEN:
        print("‚ùå HF_TOKEN not set! Please set your HF_TOKEN environment variable.")
        return
    
    # Define all models to process
    models = [
        # NYC models
        # ('NYC', 'Nrqvae-NYC-div0.00-commit0.25-lr1e-3'),
        # ('NYC', 'Nrqvae-NYC-div0.25-commit0.25-lr1e-3'),
        # ('NYC', 'Nrqvae-NYC-div0.50-commit0.25-lr1e-3'),
        # ('NYC', 'Nrqvae-NYC-div0.75-commit0.25-lr1e-3'),
        # ('NYC', 'Nrqvae-withKL-NYC-div0.25-commit0.25-lr1e-3'),
        # ('NYC', 'Nrqvae-without_L_quant-NYC-div0.25-commit0.25-lr1e-3'),
        ('NYC_Exploration', 'Nrqvae-NYC_Exploration-div0.25-commit0.25-lr1e-3'),
        # TKY models
        # ('TKY', 'Nrqvae-TKY-div0.00-commit0.25-lr1e-3'),
        # ('TKY', 'Nrqvae-TKY-div0.25-commit0.25-lr1e-3'),
        # ('TKY', 'Nrqvae-TKY-div0.50-commit0.25-lr1e-3'),
        # ('TKY', 'Nrqvae-TKY-div0.75-commit0.25-lr1e-3'),
        # ('TKY', 'Nrqvae-withKL-TKY-div0.25-commit0.25-lr1e-3'),
        # ('TKY', 'Nrqvae-without_L_quant-TKY-div0.25-commit0.25-lr1e-3'),
        ('TKY_Exploration', 'Nrqvae-TKY_Exploration-div0.25-commit0.25-lr1e-3'),
    ]
    
    print("üöÄ Batch Processing Unique SID Calculation from Codebook CSV")
    print("=" * 70)
    print(f"üìä Processing {len(models)} models from LLM Dataset/Intermediate Files/\n")
    
    results = []
    
    for i, (dataset, model) in enumerate(models, 1):
        print(f"[{i}/{len(models)}] Processing {dataset}/{model}...")
        metrics = process_model(dataset, model)
        if metrics:
            results.append((dataset, model, metrics))
            print(f"   ‚úÖ Unique SID: {metrics['unique_sid']}")
            print(f"   ‚úÖ Collided SID buckets: {metrics['collided_sid_buckets']}\n")
    
    # Print summary table
    print("=" * 70)
    print("üìà Summary Results")
    print("=" * 70)
    print(f"{'Dataset':<10} {'Model':<45} {'Unique SID':<12} {'Collided SID':<13}")
    print("-" * 80)
    
    for dataset, model, metrics in results:
        print(f"{dataset:<10} {model:<45} {metrics['unique_sid']:<12} {metrics['collided_sid_buckets']:<13}")
    
    # Save to CSV
    output_dir = Path("output") / "sid_metrics"
    output_dir.mkdir(parents=True, exist_ok=True)
    csv_file = output_dir / "unique_collision_results.csv"
    
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Dataset', 'Model', 'Unique_SID', 'Collided_SID_Buckets'])
        for dataset, model, metrics in results:
            writer.writerow([
                dataset, 
                model, 
                metrics['unique_sid'],
                metrics['collided_sid_buckets']
            ])
    
    print(f"\nüíæ Results saved to: {csv_file}")


if __name__ == "__main__":
    main()

