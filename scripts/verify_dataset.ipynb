{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e79e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vislab/poi/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from poi.dataset.llm import load_prompt_completion_llm_dataset, extract_user_sids, find_all_user_sids_in_dataset, filter_test\n",
    "from poi import settings\n",
    "\n",
    "ds_dir = settings.DATASETS_DIR / \"NYC\" / \"LLM Dataset\" / \"paper-nyc-base\"\n",
    "\n",
    "train_ds_path = ds_dir / \"train_codebook.json\"\n",
    "test_ds_path = ds_dir / \"test_codebook.json\"\n",
    "\n",
    "train_ds = load_prompt_completion_llm_dataset(train_ds_path)\n",
    "test_ds = load_prompt_completion_llm_dataset(test_ds_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a record of a user's POI accesses, your task is based on the history to predict the POI that the user is likely to access at the specified time.\n",
      "User_2 visited: <a_26><b_18><c_11><d_1> at 2012-04-20 07:23, <a_31><b_12><c_4> at 2012-04-20 21:32, <a_29><b_13><c_6> at 2012-04-20 21:34, <a_31><b_2><c_27> at 2012-04-21 13:20, <a_0><b_9><c_1> at 2012-04-21 20:45, <a_29><b_13><c_6> at 2012-04-21 22:29, <a_26><b_18><c_10><d_0> at 2012-04-23 06:23, <a_26><b_3><c_8> at 2012-04-23 06:23, <a_26><b_18><c_10><d_1> at 2012-04-23 06:23, <a_26><b_24><c_26> at 2012-04-23 07:16, <a_26><b_18><c_11><d_0> at 2012-04-23 07:17, <a_26><b_18><c_11><d_1> at 2012-04-23 07:17, <a_6><b_0><c_29> at 2012-04-23 07:17, <a_31><b_12><c_4> at 2012-04-23 10:55, <a_26><b_24><c_26> at 2012-04-24 06:59, <a_26><b_18><c_10><d_0> at 2012-04-24 06:59, <a_26><b_18><c_10><d_1> at 2012-04-24 06:59, <a_26><b_3><c_8> at 2012-04-24 06:59, <a_26><b_18><c_11><d_0> at 2012-04-24 07:57, <a_6><b_0><c_29> at 2012-04-24 07:57, <a_26><b_18><c_11><d_1> at 2012-04-24 07:58, <a_31><b_12><c_4> at 2012-04-24 17:13, <a_26><b_18><c_10><d_0> at 2012-04-25 06:46, <a_26><b_18><c_10><d_1> at 2012-04-25 06:46, <a_26><b_3><c_8> at 2012-04-25 06:46, <a_26><b_24><c_26> at 2012-04-25 08:45, <a_26><b_18><c_10><d_0> at 2012-04-25 08:46, <a_26><b_18><c_11><d_0> at 2012-04-25 08:46, <a_6><b_0><c_29> at 2012-04-25 08:46, <a_26><b_18><c_11><d_1> at 2012-04-25 08:47, <a_26><b_18><c_10><d_0> at 2012-04-26 06:44, <a_26><b_24><c_26> at 2012-04-26 06:44, <a_26><b_3><c_8> at 2012-04-26 06:44, <a_26><b_18><c_10><d_1> at 2012-04-26 06:44, <a_6><b_0><c_29> at 2012-04-26 07:40, <a_26><b_18><c_11><d_0> at 2012-04-26 07:40, <a_26><b_18><c_11><d_1> at 2012-04-26 07:41, <a_26><b_18><c_10><d_1> at 2012-04-27 06:08, <a_26><b_3><c_8> at 2012-04-27 06:09, <a_26><b_18><c_10><d_0> at 2012-04-27 06:09, <a_26><b_24><c_26> at 2012-04-27 06:50, <a_26><b_18><c_11><d_1> at 2012-04-27 06:51, <a_29><b_10><c_0> at 2012-04-27 10:14, <a_0><b_15><c_25> at 2012-04-27 10:14, <a_22><b_25><c_16> at 2012-04-27 10:23, <a_23><b_6><c_23> at 2012-04-27 10:24, <a_31><b_12><c_4> at 2012-04-28 21:53, <a_29><b_13><c_6> at 2012-04-28 21:54, <a_31><b_12><c_4> at 2012-05-01 15:16. When 2012-05-01 17:35 user_2 is likely to visit: \n",
      "User_2\n",
      "{'<a_26><b_18><c_10><d_1>', '<a_6><b_0><c_29>', '<a_0><b_15><c_25>', '<a_23><b_6><c_23>', '<a_0><b_9><c_1>', '<a_29><b_13><c_6>', '<a_26><b_18><c_11><d_1>', '<a_26><b_18><c_10><d_0>', '<a_26><b_24><c_26>', '<a_10><b_29><c_12>', '<a_31><b_12><c_4>', '<a_26><b_18><c_11><d_0>', '<a_26><b_3><c_8>', '<a_31><b_2><c_27>', '<a_22><b_25><c_16>', '<a_29><b_10><c_0>'}\n"
     ]
    }
   ],
   "source": [
    "prompt = train_ds[0][\"prompt\"]\n",
    "completion = train_ds[0][\"completion\"]\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "user, sids = extract_user_sids(train_ds[0])\n",
    "print(user)\n",
    "print(sids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07d68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in test but not in train: {'User_1067', 'User_345', 'User_125', 'User_826', 'User_371', 'User_837', 'User_170'}\n",
      "\n",
      "SIDs in test but not in train: {'<a_31><b_25><c_8><d_4>', '<a_9><b_20><c_29><d_3>', '<a_3><b_2><c_30>', '<a_6><b_4><c_25><d_1>', '<a_29><b_11><c_29>', '<a_19><b_0><c_7>', '<a_9><b_19><c_12>', '<a_17><b_17><c_12><d_3>', '<a_6><b_1><c_17><d_0>', '<a_27><b_7><c_7>', '<a_17><b_17><c_20><d_0>', '<a_28><b_17><c_8>', '<a_19><b_24><c_4><d_1>', '<a_28><b_24><c_7><d_2>', '<a_9><b_0><c_7><d_1>', '<a_24><b_7><c_3>', '<a_3><b_3><c_7><d_1>', '<a_17><b_27><c_23><d_0>', '<a_25><b_13><c_25>', '<a_26><b_6><c_13><d_1>', '<a_18><b_3><c_16><d_1>', '<a_28><b_24><c_14>', '<a_22><b_10><c_1>', '<a_28><b_24><c_7><d_1>', '<a_22><b_0><c_9><d_1>', '<a_6><b_25><c_31><d_2>', '<a_3><b_3><c_7><d_2>', '<a_3><b_29><c_7>', '<a_19><b_25><c_27>', '<a_4><b_6><c_17><d_0>', '<a_23><b_7><c_8>', '<a_7><b_17><c_26>', '<a_19><b_1><c_8><d_1>', '<a_26><b_30><c_14>', '<a_5><b_9><c_23><d_1>', '<a_27><b_31><c_13><d_1>', '<a_28><b_3><c_17>', '<a_7><b_12><c_8><d_1>', '<a_22><b_2><c_12>', '<a_3><b_3><c_3><d_1>', '<a_14><b_29><c_7><d_1>', '<a_17><b_1><c_12>', '<a_23><b_10><c_27>', '<a_3><b_18><c_22><d_1>', '<a_27><b_2><c_23><d_0>', '<a_7><b_17><c_8><d_0>', '<a_5><b_30><c_26>', '<a_5><b_9><c_29>', '<a_23><b_1><c_29>', '<a_31><b_25><c_8><d_1>', '<a_5><b_17><c_17>', '<a_22><b_27><c_14><d_1>', '<a_29><b_11><c_13><d_1>', '<a_5><b_25><c_7><d_2>', '<a_28><b_27><c_20><d_1>', '<a_19><b_5><c_26>', '<a_22><b_3><c_8><d_0>', '<a_14><b_26><c_22><d_1>', '<a_5><b_12><c_29><d_0>', '<a_28><b_27><c_23><d_0>', '<a_5><b_7><c_12><d_1>', '<a_9><b_10><c_23><d_1>', '<a_25><b_5><c_10>', '<a_31><b_20><c_16>', '<a_17><b_15><c_17><d_0>', '<a_6><b_4><c_11>', '<a_23><b_6><c_17>', '<a_5><b_19><c_17>', '<a_29><b_3><c_13><d_0>', '<a_23><b_1><c_13>', '<a_8><b_15><c_29>', '<a_29><b_12><c_27>', '<a_9><b_10><c_21><d_1>', '<a_27><b_27><c_4>'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_users, train_sids = find_all_user_sids_in_dataset(train_ds)\n",
    "test_users, test_sids = find_all_user_sids_in_dataset(test_ds)\n",
    "\n",
    "print(\"Users in test but not in train:\", test_users - train_users)\n",
    "print()\n",
    "\n",
    "print(\"SIDs in test but not in train:\", test_sids - train_sids)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in filtered test but not in train: set()\n",
      "\n",
      "SIDs in filtered test but not in train: set()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_test_ds = test_ds.filter(filter_test, fn_kwargs={\"filter_users\": test_users - train_users, \"filter_sids\": test_sids - train_sids})\n",
    "filtered_users, filtered_sids = find_all_user_sids_in_dataset(filtered_test_ds)\n",
    "\n",
    "print(\"Users in filtered test but not in train:\", filtered_users - train_users)\n",
    "print()\n",
    "print(\"SIDs in filtered test but not in train:\", filtered_sids - train_sids)\n",
    "print()\n",
    "\n",
    "print(len(filtered_test_ds) + \"/\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80678e40",
   "metadata": {},
   "source": [
    "### Try to evaluate on filtered test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b5121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.516 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.9 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 668/668 [04:08<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.3338323353293413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from poi.llm.evaluate import top_one_accuracy\n",
    "from poi.llm import LLMConfig, load_fast_inference_model\n",
    "from poi.dataset.llm import load_prompt_completion_llm_dataset, extract_user_sids, find_all_user_sids_in_dataset, filter_test\n",
    "from poi import settings\n",
    "\n",
    "ds_dir = settings.DATASETS_DIR / \"NYC\" / \"LLM Dataset\" / \"paper-nyc-base\"\n",
    "\n",
    "train_ds_path = ds_dir / \"train_codebook.json\"\n",
    "test_ds_path = ds_dir / \"test_codebook.json\"\n",
    "\n",
    "train_ds = load_prompt_completion_llm_dataset(train_ds_path)\n",
    "test_ds = load_prompt_completion_llm_dataset(test_ds_path)\n",
    "\n",
    "train_users, train_sids = find_all_user_sids_in_dataset(train_ds)\n",
    "test_users, test_sids = find_all_user_sids_in_dataset(test_ds)\n",
    "\n",
    "test_ds = test_ds.filter(filter_test, fn_kwargs={\"filter_users\": test_users - train_users, \"filter_sids\": test_sids - train_sids})\n",
    "\n",
    "\n",
    "config = LLMConfig(run_name=\"llama3-nyc-test\")\n",
    "model = load_fast_inference_model(config)\n",
    "\n",
    "acc = top_one_accuracy(config, model, test_ds)\n",
    "print(f\"Top-1 accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec8bd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vislab/poi/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.516 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.9 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 793/793 [01:53<00:00,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.31273644388398486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from poi.llm.evaluate import top_one_accuracy\n",
    "from poi.llm import LLMConfig, load_fast_inference_model\n",
    "from poi.dataset.llm import load_prompt_completion_llm_dataset, extract_user_sids, find_all_user_sids_in_dataset, filter_test\n",
    "from poi import settings\n",
    "\n",
    "ds_dir = settings.DATASETS_DIR / \"NYC\" / \"LLM Dataset\" / \"ablation without SID\"\n",
    "\n",
    "train_ds_path = ds_dir / \"train_id.json\"\n",
    "test_ds_path = ds_dir / \"test_id.json\"\n",
    "\n",
    "train_ds = load_prompt_completion_llm_dataset(train_ds_path)\n",
    "test_ds = load_prompt_completion_llm_dataset(test_ds_path)\n",
    "\n",
    "train_users, train_sids = find_all_user_sids_in_dataset(train_ds)\n",
    "test_users, test_sids = find_all_user_sids_in_dataset(test_ds)\n",
    "\n",
    "test_ds = test_ds.filter(filter_test, fn_kwargs={\"filter_users\": test_users - train_users, \"filter_sids\": test_sids - train_sids})\n",
    "\n",
    "\n",
    "config = LLMConfig(run_name=\"llama3-nyc-test-no-sid\")\n",
    "model = load_fast_inference_model(config)\n",
    "\n",
    "acc = top_one_accuracy(config, model, test_ds, is_sid=False)\n",
    "print(f\"Top-1 accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee82169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poi.llm.evaluate import top_one_accuracy\n",
    "from poi.llm import LLMConfig, load_fast_inference_model\n",
    "from poi.dataset.llm import load_prompt_completion_llm_dataset, extract_user_sids, find_all_user_sids_in_dataset, filter_test\n",
    "from poi import settings\n",
    "\n",
    "ds_dir = settings.DATASETS_DIR / \"NYC\" / \"LLM Dataset\" / \"paper-nyc-base\"\n",
    "\n",
    "train_ds_path = ds_dir / \"train_codebook.json\"\n",
    "test_ds_path = ds_dir / \"test_codebook.json\"\n",
    "\n",
    "train_ds = load_prompt_completion_llm_dataset(train_ds_path)\n",
    "test_ds = load_prompt_completion_llm_dataset(test_ds_path)\n",
    "\n",
    "train_users, train_sids = find_all_user_sids_in_dataset(train_ds)\n",
    "test_users, test_sids = find_all_user_sids_in_dataset(test_ds)\n",
    "\n",
    "test_ds = test_ds.filter(filter_test, fn_kwargs={\"filter_users\": test_users - train_users, \"filter_sids\": test_sids - train_sids})\n",
    "\n",
    "\n",
    "config = LLMConfig(run_name=\"llama3-nyc-base\")\n",
    "model = load_fast_inference_model(config)\n",
    "\n",
    "acc = top_one_accuracy(config, model, test_ds)\n",
    "print(f\"Top-1 accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014215d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vislab/poi/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m train_ds = load_prompt_completion_llm_dataset(train_ds_path)\n\u001b[32m     12\u001b[39m test_ds = load_prompt_completion_llm_dataset(test_ds_path)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m train_users, train_sids = \u001b[43mfind_all_user_sids_in_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m test_users, test_sids = find_all_user_sids_in_dataset(test_ds)\n\u001b[32m     17\u001b[39m test_ds = test_ds.filter(filter_test, fn_kwargs={\u001b[33m\"\u001b[39m\u001b[33mfilter_users\u001b[39m\u001b[33m\"\u001b[39m: test_users - train_users, \u001b[33m\"\u001b[39m\u001b[33mfilter_sids\u001b[39m\u001b[33m\"\u001b[39m: test_sids - train_sids})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/poi/src/poi/dataset/llm.py:94\u001b[39m, in \u001b[36mfind_all_user_sids_in_dataset\u001b[39m\u001b[34m(ds)\u001b[39m\n\u001b[32m     92\u001b[39m sids = \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m]()\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m ds:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     user, sids_in_record = \u001b[43mextract_user_sids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     users.add(user)\n\u001b[32m     96\u001b[39m     sids.update(sids_in_record)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/poi/src/poi/dataset/llm.py:83\u001b[39m, in \u001b[36mextract_user_sids\u001b[39m\u001b[34m(record)\u001b[39m\n\u001b[32m     80\u001b[39m prompt = record[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     81\u001b[39m completion = record[\u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m user: \u001b[38;5;28mstr\u001b[39m = \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUSER_PATTERN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     84\u001b[39m prompt_sids: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = re.findall(SID_PATTERN, prompt)\n\u001b[32m     85\u001b[39m completion_sids: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = re.findall(SID_PATTERN, completion)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "from poi.llm.evaluate import top_one_accuracy\n",
    "from poi.llm import LLMConfig, load_fast_inference_model\n",
    "from poi.dataset.llm import load_prompt_completion_llm_dataset, extract_user_sids, find_all_user_sids_in_dataset, filter_test\n",
    "from poi import settings\n",
    "\n",
    "ds_dir = settings.DATASETS_DIR / \"NYC\" / \"LLM Dataset\" / \"ablation without Time\"\n",
    "\n",
    "train_ds_path = ds_dir / \"train_codebook_notime.json\"\n",
    "test_ds_path = ds_dir / \"test_codebook_notime.json\"\n",
    "\n",
    "train_ds = load_prompt_completion_llm_dataset(train_ds_path)\n",
    "test_ds = load_prompt_completion_llm_dataset(test_ds_path)\n",
    "\n",
    "train_users, train_sids = find_all_user_sids_in_dataset(train_ds)\n",
    "test_users, test_sids = find_all_user_sids_in_dataset(test_ds)\n",
    "\n",
    "test_ds = test_ds.filter(filter_test, fn_kwargs={\"filter_users\": test_users - train_users, \"filter_sids\": test_sids - train_sids})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf461f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_user_sids(record: dict[str, str]) -> tuple[str, set[str]]:\n",
    "    SID_PATTERN = r\"(?:<\\w+>)+\"\n",
    "    USER_PATTERN = r\"user_\\d+\"\n",
    "\n",
    "    prompt = record[\"prompt\"].lower()\n",
    "    completion = record[\"completion\"].lower()\n",
    "\n",
    "    user: str = re.findall(USER_PATTERN, prompt)[0]\n",
    "    prompt_sids: list[str] = re.findall(SID_PATTERN, prompt)\n",
    "    completion_sids: list[str] = re.findall(SID_PATTERN, completion)\n",
    "\n",
    "    sids = set(prompt_sids) | set(completion_sids)\n",
    "    return user, sids\n",
    "\n",
    "for i in range(len(train_ds)):\n",
    "    try:\n",
    "        user, sids = extract_user_sids(train_ds[i])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(train_ds[i])\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
